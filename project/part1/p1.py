"""Final project, part 1
Igor Adamski, CID 01069152
"""
import numpy as np
import matplotlib.pyplot as plt
from m1 import part1 #assumes cost.f90 and p1.f90 have been compiled with f2py into module, m1.xxx.so
from m1 import cost

def bdg_analyze():
    """ analyze effectiveness of global b-d method

    ANALYSIS:

    On the first figure generated by the code, we can see the comparison of the minima that
    bracket descent and global bracket descent converge to. I also vary the parameters of
    global bracket descent to explore the hyperparameter space (a bit). I am showing the minima
    for 5 different xguesses chosen so that together with the parameters for global bracket
    descent the xguesses for the centroids stay in the region 0<x1<1000, 0<x2<1000.
    On the first figure we can see that firstly, alpha = 0.1 produces better minima
    than alpha = 0.9. This can be explained by the fact that as the agressiveness of shifty towards
    the lowst cost is higher, global bracket descent loses its ability to explore the space of
    the function that much as all the centroid come more closer together. With a lower agressiveness parameter
    the centroids stay more spread out and thus can run into local minima that are better. Secondly,
    we can see that for d=1, nb=10 and for d=5 nb=10, global bracket descent converges to the same
    minima as bracket descent. This is not surprising because for these parameters the initial guesses are not so spread
    out and hence global bd cannot explore the cost very far from the initial guess and thus is not able
    to find better local minima. Also becase nb=10, the number of centroids that are iterated is not big enough
    to contribute to the minimum search. We can see the effect of increasing nb from 10 to 50, as global bd
    almost always finds a better minima than bracket descent. For d=1 the performance of the nb=50 global bd
    is less visible, although on 3 xguesses out of 5 global bd finds a better minimum. For d=5 and alpha=0.1, the difference is
    huge. Global bd for nb=50 and d=5 finds a lot better minima, up to 6 times lower for all the xguesses. The increase in
    performance is not that visible for the alpha=0.9 case, when global bd finds better minima but not by such a margin.
    So in summary, the bigger the nb and d and the smaller the alpha, the better the minima that are found in global bd.

    In the second figure we can see the number of iterations that global bd and bracket descent take, with the same xguesses
    and varying parameters as above. The graph doesnt produce straight forward and clear results. One thing that we can observe
    is that global bracket descent always takes the same or more iterations as bracket descent to converge. This is not surprising as
    global bd is just a version of bracket descent so we expect it to have similar number of iterations. For the case when
    d=1 and nb=10, global bd a bit more iterations than bracket descent, but as we saw above, converges to equal minima. This extra number
    of iterations is due to the fact that global bd needs to converge all its centroids to the minimum, regardless if one of them
    already sits in the minimum. For the nb=50 case, global bd takes much more iterations, sometimes hitting the maxit=1000 mark but as we saw
    above, it produces better minima. It is also visible that on those xguesses that gave the nb=50 (d=1) global bd a lower minima, the number
    of iterations always hits the 1000 ceiling, whereas if it produced similar minima then the number of iterations is slightly bigger than for
    bracket descent. For the case when d=10, we can see that for two xguesses the nb=10 case hits the maxit iterations and we can expect that
    for these two xguesses it found a better minimum than bracket descent (but it was not visible on the previous figure because the nb=50 case had
    so much better minima that the marginal difference was not seen). We can also see that the purple line (one corresponding to the case d=5, nb=50)
    is constantly at the ceiling 1000 which goes in line with the previous figure because for these parameters global bd found the best minima.
    We can see that for the case alpha=0.9 the difference in number of iterations done is a lot smaller, with the exception of some cases when the
    number of iterations hits the 1000 ceiling. For the case d=1 nb=10 and d=1 nb=50 the number of iterations is almost identical to bracket descent
    with the exception of one xguess when the latter case find a better minimum.

    On the third figure we can see an example of the paths of bracket descent and global bracket descent for the case when global bd finds a
    slightly better minimum than bracket descent and hits its ceiling of 1000 iterations. We can see clearly that the paths of these two algorithms
    are different but that they meet in the bracket descent minimum but global bracket descent manages to escape that minimum and converge
    to another one. This is the effect of nb=50 because we can see that global bd was inclined in the direction of its minimum before finding itself
    in the bracket descent minimum. What happened was that some of the centroids were going in the direction of the global bd minimum and then
    some centroids found the bracket descent minimum but then the other centroids pulled all the other centroids toward the global bd minimum.
    We can also see that global bd takes bigger steps (so more steps in the same direction) than bracket descent.
    """

    part1.itermax = 1000
    xguess = [[200,200], [200,700], [500,500], [700,700], [700,200]]
    nb = [10, 50]
    d = [1, 5]
    alpha = [0.1, 0.9]

    glob_1 = np.zeros([len(xguess), len(nb), len(d)])
    glob_2 = np.zeros([len(xguess), len(nb), len(d)])
    glob_1_its = np.zeros([len(xguess), len(nb), len(d)])
    glob_2_its = np.zeros([len(xguess), len(nb), len(d)])
    sing = np.zeros([len(xguess)])
    sing_its = np.zeros([len(xguess)])
    i = 0
    j = 0
    k = 0

    for xg in xguess:
        sing[i] = part1.bracket_descent(xg)[1]
        sing_its[i] = len(part1.jpath)
        if xg == [500,500]:
            bd_path = part1.xpath.copy()

        j = 0
        for n in nb:
            k = 0
            for di in d:
                glob_1[i, j, k] = part1.bdglobal(xg,n,di,alpha[0])[1]
                glob_1_its[i, j, k] = len(part1.jpath)
                if xg == [500,500] and n == 50 and di == 1:
                    glob_path = part1.xpath.copy()
                k += 1
            j += 1
        i += 1

    i = 0
    j = 0
    k = 0
    for xg in xguess:
        j = 0
        for n in nb:
            k = 0
            for di in d:
                glob_2[i, j, k] = part1.bdglobal(xg,n,di,alpha[1])[1]
                glob_2_its[i, j, k] = len(part1.jpath)
                k += 1
            j += 1
        i += 1


    x = np.arange(5)
    fig = plt.figure(figsize=(12,8))
    fig.suptitle('Figures showing the final cost values of the bracket descent method \n and the global bracket descent with varying parameters. bdg_analyze(), Igor Adamski')
    ax = fig.add_subplot(221)
    ax2 = fig.add_subplot(222)
    ax3 = fig.add_subplot(223)
    ax4 = fig.add_subplot(224)
    ax.bar(np.array(x), sing, width=0.2, color='red', align='center', label='bracket_descent')
    ax.plot(x, glob_1[:,0,0], color='blue', label='d=1,nb=10')
    ax.plot(x, glob_1[:,1,0], color='purple', label='d=1,nb=50')
    ax.set_ylabel('Value of cost')
    ax2.bar(np.array(x), sing, width=0.2, color='red', align='center', label='bracket_descent')
    ax2.plot(x, glob_1[:,0,1], color='blue', label='d=5,nb=10')
    ax2.plot(x, glob_1[:,1,1], color='purple', label='d=5,nb=50')
    ax2.set_ylabel('Value of cost')
    ax3.bar(np.array(x), sing, width=0.2, color='red', align='center', label='bracket_descent')
    ax3.plot(x, glob_2[:,0,0], color='blue', label='d=1,nb=10')
    ax3.plot(x, glob_2[:,1,0], color='purple', label='d=1,nb=50')
    ax3.set_ylabel('Value of cost')
    ax4.bar(np.array(x), sing, width=0.2, color='red', align='center', label='bracket_descent')
    ax4.plot(x, glob_2[:,0,1], color='blue', label='d=5,nb=10')
    ax4.plot(x, glob_2[:,1,1], color='purple', label='d=5,nb=50')
    ax4.set_ylabel('Value of cost')
    ax.set_title(r'$\alpha$ = {}'.format(alpha[0]))
    ax.set_xticks(x)
    ax.set_xticklabels(xguess)
    ax2.set_title(r'$\alpha$ = {}'.format(alpha[0]))
    ax2.set_xticks(x)
    ax2.set_xticklabels(xguess)
    ax.legend()
    ax2.legend()
    ax3.set_title(r'$\alpha$ = {}'.format(alpha[1]))
    ax3.set_xticks(x)
    ax3.set_xticklabels(xguess)
    ax4.set_title(r'$\alpha$ = {}'.format(alpha[1]))
    ax4.set_xticks(x)
    ax4.set_xticklabels(xguess)
    ax3.legend()
    ax4.legend()

    fig2 = plt.figure(figsize=(12,8))
    fig2.suptitle('Figures showing the number of iterations it takes to converge for bracket descent method \n and the global bracket descent with varying parameters. bdg_analyze(), Igor Adamski')
    ax = fig2.add_subplot(221)
    ax2 = fig2.add_subplot(222)
    ax3 = fig2.add_subplot(223)
    ax4 = fig2.add_subplot(224)
    ax.bar(np.array(x), sing_its, width=0.2, color='red', align='center', label='bracket_descent')
    ax.plot(x, glob_1_its[:,0,0], color='blue', label='d=1,nb=10')
    ax.plot(x, glob_1_its[:,1,0], color='purple', label='d=1,nb=50')
    ax.set_ylabel('Number of iterations to convergence')
    ax2.bar(np.array(x), sing_its, width=0.2, color='red', align='center', label='bracket_descent')
    ax2.plot(x, glob_1_its[:,0,1], color='blue', label='d=10,nb=10')
    ax2.plot(x, glob_1_its[:,1,1], color='purple', label='d=10,nb=50')
    ax2.set_ylabel('Number of iterations to convergence')
    ax3.bar(np.array(x), sing_its, width=0.2, color='red', align='center', label='bracket_descent')
    ax3.plot(x, glob_2_its[:,0,0], color='blue', label='d=1,nb=10')
    ax3.plot(x, glob_2_its[:,1,0], color='purple', label='d=1,nb=50')
    ax3.set_ylabel('Number of iterations to convergence')
    ax4.bar(np.array(x), sing_its, width=0.2, color='red', align='center', label='bracket_descent')
    ax4.plot(x, glob_2_its[:,0,1], color='blue', label='d=10,nb=10')
    ax4.plot(x, glob_2_its[:,1,1], color='purple', label='d=10,nb=50')
    ax4.set_ylabel('Number of iterations to convergence')
    ax.set_xticks(x)
    ax.set_xticklabels(xguess)
    ax.set_title(r'$\alpha$ = {}'.format(alpha[0]))
    ax2.set_xticks(x)
    ax2.set_xticklabels(xguess)
    ax2.set_title(r'$\alpha$ = {}'.format(alpha[0]))
    ax.legend()
    ax2.legend()
    ax3.set_xticks(x)
    ax3.set_xticklabels(xguess)
    ax3.set_title(r'$\alpha$ = {}'.format(alpha[1]))
    ax4.set_xticks(x)
    ax4.set_xticklabels(xguess)
    ax4.set_title(r'$\alpha$ = {}'.format(alpha[1]))
    ax3.legend()
    ax4.legend()

    fig3 = plt.figure(figsize=(8,6))
    ax = fig3.add_subplot(111)
    ax.plot(bd_path[:,0], bd_path[:,1], label='Bracket descent path')
    ax.plot(glob_path[:,0], glob_path[:,1], label='Global bracket descent path')
    ax.plot(500,500,'v', label='Starting guess')
    ax.plot(glob_path[len(glob_path[:,0])-1,0], glob_path[len(glob_path[:,0])-1,1], '^', label='Final minimum of global bracket descent')
    ax.plot(bd_path[len(bd_path[:,0])-1,0], bd_path[len(bd_path[:,0])-1,1], '^', label='Final minimum of bracket descent')
    ax.set_title('Paths of global bracket descent and bracket descent starting from [500,500]. \n Global bd has parameters nb=50, d=1, alpha=0.1. bdg_analyze(), Igor Adamski')
    ax.legend()

    plt.show()





if __name__=='__main__':
    #Modify input/output as needed to generate final figures with call(s) below
    bdg_analyze()
